{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the number of words spoken by male and female characters in calderon's comedias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_file = 'calderon-gender-prediction/all_characters.csv'\n",
    "character_df = pd.read_csv(character_file, usecols = ['id','genre','character_gender','character_id', 'scenes', 'utterances', 'tokens', 'words_spoken'])\n",
    "\n",
    "#only examine comedias files, not autos, loas, or zarzuelas\n",
    "comedias_df = character_df[(character_df['genre'] != 'auto sacramental') & \n",
    "                          (character_df['genre'] != 'loa') & \n",
    "                          (character_df['genre'] != 'zarzuela') & \n",
    "                          (character_df['genre'] != 'mojiganga')]\n",
    "\n",
    "#drop nan values\n",
    "\n",
    "comedias_df = comedias_df.dropna()\n",
    "comedias_df = comedias_df[comedias_df['words_spoken'] > 30]\n",
    "print(comedias_df.shape)\n",
    "print(comedias_df['genre'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comedias_df['id'].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chart the number of words spoken by male and female characters\n",
    "for i in comedias_df['character_gender'].unique():\n",
    "    filtered_data = comedias_df[comedias_df['character_gender'] == i]\n",
    "    plt.hist(filtered_data['words_spoken'], label=i)\n",
    "    plt.title('Number of Words Spoken by Character Gender')\n",
    "    plt.xlabel('Number of Tokens Spoken')\n",
    "    plt.ylabel('Number of Characters')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the total number of words spoken by men and by women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the sum of the words spoken coulmn for each \n",
    "print(comedias_df.groupby('character_gender')['words_spoken'].sum())\n",
    "\n",
    "#print how many characters\n",
    "print(comedias_df.groupby('character_gender')['character_id'].nunique()) #but some names are repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many characters in the corpus speak more than 30 words\n",
    "print(comedias_df[comedias_df['words_spoken'] > 30].shape)\n",
    "\n",
    "#count number of rows in the dataframe with male or female and > 30 words spoken\n",
    "print(comedias_df[comedias_df['words_spoken'] > 30]['character_gender'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the max, min, and mean of the number of words spoken by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in comedias_df['character_gender'].unique():\n",
    "    filtered_data = comedias_df[comedias_df['character_gender'] == i]\n",
    "    print(i)\n",
    "    print(filtered_data['words_spoken'].mean())\n",
    "    print(filtered_data['words_spoken'].median())\n",
    "    print(filtered_data['words_spoken'].std())\n",
    "    print(filtered_data['words_spoken'].min())\n",
    "    print(filtered_data['words_spoken'].max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print proportion of rows where df['is_male'] == df['predictions']\n",
    "def proportion_correct(tokens_df, row_of_interest = 'predictions'):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    male_correct = 0\n",
    "    female_correct = 0\n",
    "    male_incorrect = 0\n",
    "    female_incorrect = 0\n",
    "\n",
    "    for index, row in tokens_df.iterrows():\n",
    "        if row['is_male'] == row[row_of_interest]:\n",
    "            correct += 1\n",
    "            if row['is_male'] == 1:\n",
    "                male_correct += 1\n",
    "            else:\n",
    "                female_correct += 1\n",
    "\n",
    "        else:\n",
    "            if row['is_male'] == 1:\n",
    "                male_incorrect += 1\n",
    "            \n",
    "            else:\n",
    "                female_incorrect += 1\n",
    "                \n",
    "\n",
    "        total += 1\n",
    "\n",
    "    if female_correct!=0 and male_correct!=0: #(male_correct + male_incorrect) != 0 and (female_correct + female_incorrect) != 0 and (male_correct + female_incorrect) !=0 and (female_correct + male_incorrect) !=0 and \n",
    "        m_precision = male_correct/(male_correct + female_incorrect)\n",
    "        m_recall = male_correct / (male_correct + male_incorrect)\n",
    "        m_f1 = 2 * (m_precision * m_recall) / (m_precision + m_recall)\n",
    "        print('Male F1: ',m_f1)\n",
    "\n",
    "        f_precision = female_correct / (female_correct + male_incorrect)\n",
    "        f_recall = female_correct / (female_correct + female_incorrect)\n",
    "        f_f1 = 2 * (f_precision * f_recall) / (f_precision + f_recall)\n",
    "        print('Female F1: ',f_f1)\n",
    "\n",
    "        average_precision = (m_precision + f_precision) / 2\n",
    "        average_recall = (m_recall + f_recall) / 2\n",
    "        average_f1 = (m_f1 + f_f1) / 2\n",
    "        print('Average Precision: ', average_precision)\n",
    "        print('Average Recall: ', average_recall)\n",
    "        print('Average F1: ', average_f1)\n",
    "    else:\n",
    "        print('Model made no correct predicitons for one class')\n",
    "        average_precision = 0\n",
    "        average_recall = 0\n",
    "        average_f1 = 0\n",
    "    return average_precision, average_recall, average_f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_numbers(input_string):\n",
    "    numbers = input_string.strip('[]').split()\n",
    "    numbers = [float(num) for num in numbers]\n",
    "    return numbers\n",
    "\n",
    "def geometric_mean_probability(df):\n",
    "    result_list = []\n",
    "\n",
    "    for (character_id, play_id), group in df.groupby(['character_id', 'id']):\n",
    "        # Extract the first and second numbers after '[' and ']'\n",
    "        probabilities = group['probabilities'].apply(lambda x: convert_string_to_numbers(x)[0])  # First number\n",
    "        second_probabilities = group['probabilities'].apply(lambda x: convert_string_to_numbers(x)[1])  # Second number\n",
    "\n",
    "\n",
    "        # Use the mean of the second probabilities for is_male == 1\n",
    "        if group['is_male'].iloc[0] == 1:\n",
    "            probabilities = second_probabilities\n",
    "\n",
    "        geometric_mean_prob = probabilities.prod() ** (1 / len(probabilities))\n",
    "        mean_actual = group['is_male'].mean()\n",
    "\n",
    "        # Round the mean prediction to either 0 or 1\n",
    "        mean_predict = round(geometric_mean_prob)\n",
    "\n",
    "        #column_name = group[column_name].iloc[0]\n",
    "\n",
    "        result_list.append({\n",
    "            'id': play_id,\n",
    "            'character_id': character_id,\n",
    "            'geometric_mean_probability': geometric_mean_prob,\n",
    "            'is_male': mean_actual,\n",
    "            'geo_predictions': mean_predict,\n",
    "            'average_prediction' : group['predictions'].mean()\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame(result_list)\n",
    "    return result_df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall, and F1-score for each level of text input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Level\n",
    "All lines a character speaks in a play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df = pd.read_csv('/calderon-gender-prediction/results/tokens_bert-base-spanish-wwm-cased_1e-05_24_5.csv')\n",
    "print(\"Character Level Predictions\")\n",
    "proportion_correct(tokens_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene Level\n",
    "All lines spoken by a character in a scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_df = pd.read_csv('/calderon-gender-prediction/results/scenes_bert-base-spanish-wwm-cased_1e-05_32_12.csv')\n",
    "print(\"Scenes Predictions\")\n",
    "proportion_correct(scenes_df)\n",
    "\n",
    "print(\"Scenes Mean Predictions\")\n",
    "geo_mean = geometric_mean_probability(scenes_df)\n",
    "\n",
    "proportion_correct(geo_mean, 'average_prediction')\n",
    "\n",
    "print(\"Scenes Geometric Mean Predictions\")\n",
    "proportion_correct(geo_mean, 'geo_predictions')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utterances\n",
    "Each line spoken by a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_df = pd.read_csv('/calderon-gender-prediction/results/utterances_bert-base-spanish_1e-05_32_14.csv')\n",
    "\n",
    "print(\"Utterances Predictions\")\n",
    "proportion_correct(utterances_df)\n",
    "\n",
    "print(\"Utterances Mean Predictions\")\n",
    "geo_mean = geometric_mean_probability(utterances_df)\n",
    "proportion_correct(geo_mean, 'average_prediction')\n",
    "\n",
    "print(\"Utterances Geometric Mean Predictions\")\n",
    "proportion_correct(geo_mean, 'geo_predictions')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert probabilites to single values rather than touples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_probabilities(df):\n",
    "    df['probabilities'] = df['probabilities'].apply(lambda x: (convert_string_to_numbers(x))[0])\n",
    "    for index, row in df.iterrows():\n",
    "        if row['probabilities'] < .5:        \n",
    "            df.loc[index,'probabilities'] = 1 - row['probabilities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_probabilities(masked_tokens_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see the most confident wrong predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find columns where is_male != predictions and probabilities corresponding to predicitons is extreme \n",
    "\n",
    "def find_misclassified(df):\n",
    "    misclassified_df = df[df['is_male'] != df['predictions']]\n",
    "\n",
    "    print(misclassified_df['character_gender'].value_counts())\n",
    "    print(misclassified_df['words_spoken'].mean())\n",
    "    print(misclassified_df['words_spoken'].std())\n",
    "    \n",
    "    return misclassified_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_df = find_misclassified(masked_tokens_df)\n",
    "misclassified_df = misclassified_df[misclassified_df['words_spoken'] > 512]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to see if there's a relationship between the number of words spoken by a character and the model's confidence in its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masked_tokens_df['words_spoken'].mean())\n",
    "print(misclassified_df['words_spoken'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masked_tokens_df['character_gender'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Misclassified Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_male = misclassified_df[misclassified_df['character_gender']=='MALE']\n",
    "misclassified_male = misclassified_male[[\"id\",\"genre\",\"character_id\",\"character_gender\",\"words_spoken\",\"tokens\",\"probabilities\"]]\n",
    "misclassified_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_female = misclassified_df[misclassified_df['character_gender']=='FEMALE']\n",
    "misclassified_female = misclassified_female[[\"id\",\"genre\",\"character_id\",\"character_gender\",\"words_spoken\",\"tokens\",\"probabilities\"]]\n",
    "misclassified_female"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Model Confidence vs. Number of Words Spoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of the number of words and the probability\n",
    "\n",
    "def probabilites_by_words_spoken(df):\n",
    "\n",
    "    #change the color of the points based on if the prediction was correct or not\n",
    "    colors = []\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if the prediction matches the ground truth label\n",
    "        if row['predictions'] == row['is_male']:\n",
    "            colors.append('green')  # Correct prediction\n",
    "        else:\n",
    "            colors.append('red')   # Incorrect prediction\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    plt.scatter(df['words_spoken'], df['probabilities'], c=colors)\n",
    "    plt.xlabel('Number of Words Spoken')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Probability of Model Prediction by Number of Words Spoken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilites_by_words_spoken(masked_tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Add character names of main characters to the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of the number of words and the probability\n",
    "\n",
    "def probabilites_by_words_spoken_main(df):\n",
    "    main_df = df[df['words_spoken'] > 2000] \n",
    "    #change the color of the points based on if the prediction was correct or not\n",
    "    colors = []\n",
    "    for index, row in main_df.iterrows():\n",
    "        # if row['words_spoken'] > 1000:\n",
    "            # Check if the prediction matches the ground truth label\n",
    "        if row['predictions'] == row['is_male']:\n",
    "            colors.append('green')  # Correct prediction\n",
    "        else:\n",
    "            colors.append('red')   # Incorrect prediction\n",
    "\n",
    "        # add label to the points row['character_id']\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    plt.scatter(main_df['words_spoken'], main_df['probabilities'], c=colors)\n",
    "    plt.xlabel('Number of Words Spoken')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Probability of Model Prediction by Number of Words Spoken')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ax = main_df.plot(x='words_spoken',y='probabilities',kind='scatter',figsize=(10,10), c=colors)\n",
    "\n",
    "    #rotate text by 45 degrees  \n",
    "\n",
    "    main_df[['words_spoken','probabilities','character_id']].apply(lambda x: ax.text(*x),axis=1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High confidence is due to the nature of cross-entropy loss\n",
    "so basically, the model is usually confident, but less confident when there are fewer words spoken, and accuracy increases the more words that are spoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilites_by_words_spoken_main(masked_tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of the number of words and the probability\n",
    "\n",
    "def probabilites_by_words_spoken_female(df):\n",
    "    main_df = df[(df['words_spoken'] > 2000)  & (df['probabilities'] > .50) & (df['is_male'] == 0)]\n",
    "    print(main_df['probabilities'].describe()) \n",
    "    \n",
    "    #change the color of the points based on if the prediction was correct or not\n",
    "    colors = []\n",
    "    for index, row in main_df.iterrows():\n",
    "        # if row['words_spoken'] > 1000:\n",
    "            # Check if the prediction matches the ground truth label\n",
    "        if row['predictions'] == row['is_male']:\n",
    "            colors.append('green')  # Correct prediction\n",
    "        else:\n",
    "            colors.append('red')   # Incorrect prediction\n",
    "\n",
    "        # add label to the points row['character_id']\n",
    "\n",
    "    ax = main_df.plot(x='words_spoken',y='probabilities',kind='scatter',figsize=(10,10), c=colors)\n",
    "\n",
    "    #rotate text by 45 degrees  \n",
    "\n",
    "    main_df[['words_spoken','probabilities','character_id']].apply(lambda x: ax.text(*x, size='small',rotation=40),axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilites_by_words_spoken_female(masked_tokens_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model's most confident predictions for male and female characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_correct_df = masked_tokens_df[masked_tokens_df['is_male'] == masked_tokens_df['predictions']]\n",
    "\n",
    "male_correct = masked_correct_df[masked_correct_df['is_male'] == 1]\n",
    "female_correct = masked_correct_df[masked_correct_df['is_male']== 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Most \"Male\" characters\n",
    "of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the 10 rows with the highest probability\n",
    "male_correct = male_correct[male_correct['words_spoken'] > 512]\n",
    "male_correct = male_correct[[\"id\",\"character_id\",\"character_gender\",\"words_spoken\",\"tokens\",\"probabilities\"]]\n",
    "male_correct.nlargest(10, 'probabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the 10 rows with the highest probability\n",
    "female_correct = female_correct[female_correct['words_spoken'] > 512]\n",
    "female_correct = female_correct[[\"id\",\"character_id\",\"character_gender\",\"words_spoken\",\"tokens\",\"probabilities\"]]\n",
    "female_correct.nlargest(10, 'probabilities')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Male & Female Characters after masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the 10 rows with the highest probability\n",
    "print(male_correct.nlargest(10, 'probabilities'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(female_correct.nlargest(10, 'probabilities'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot of scenes probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of words spoken in each scene\n",
    "masked_scenes_df['words_spoken'] = masked_scenes_df['scenes'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_probabilities(masked_scenes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_scenes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at this scatter plot for indivudual characters\n",
    "rosaura = masked_scenes_df[masked_scenes_df['character_id'] == 'rosaura']\n",
    "\n",
    "probabilites_by_words_spoken(rosaura)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female = masked_scenes_df[masked_scenes_df[\"is_male\"]== 0]\n",
    "male = masked_scenes_df[masked_scenes_df[\"is_male\"]== 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(true_positive, false_positive):\n",
    "    #calculate the precision of the model\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    return precision\n",
    "    #true positive = the number of correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(true_positive, false_negative):\n",
    "    #calculate the recall of the model\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(precision, recall):\n",
    "    #calculate the f1 score of the model\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_positive, total):\n",
    "    #calculate the accuracy of the model\n",
    "    accuracy = true_positive / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female\n",
    "true_positive = len(female[female['is_male']==female['predictions']])\n",
    "total = len(female)\n",
    "accuracy_female = accuracy(true_positive, total)\n",
    "accuracy_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male \n",
    "true_positive = len(male[male['is_male']==male['predictions']])\n",
    "total = len(male)\n",
    "accuracy_male = accuracy(true_positive, total)\n",
    "accuracy_male"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input size as a confounding variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add varialbe length of speech\n",
    "masked_scenes_df['length'] = masked_scenes_df['scenes'].apply(lambda x: len(x.split()))\n",
    "\n",
    "#if the input is more than 512 tokens, the model will not be able to process it, so change all values greater than 512 to 512\n",
    "masked_scenes_df['length'] = masked_scenes_df['length'].apply(lambda x: 512 if x > 512 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_scenes_df['quartiles'] = pd.qcut(masked_scenes_df['length'], q=4, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiles = masked_scenes_df.groupby('quartiles')\n",
    "\n",
    "quartiles.describe().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq1 = masked_scenes_df[masked_scenes_df['quartiles'] == 0]\n",
    "sq2 = masked_scenes_df[masked_scenes_df['quartiles'] == 1]\n",
    "sq3 = masked_scenes_df[masked_scenes_df['quartiles'] == 2]\n",
    "sq4 = masked_scenes_df[masked_scenes_df['quartiles'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_correct(sq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_correct(sq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_correct(sq3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_correct(sq4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average probability Cross-dressing vs non cross-dressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_tokens_df\n",
    "# cross_dressed = ['lindabridis', 'claridiana', 'rosaura', 'eugenia', 'sem√≠ramis']\n",
    "cross_dressed = masked_tokens_df.head(5)\n",
    "cross_dressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_dressed.describe().head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
